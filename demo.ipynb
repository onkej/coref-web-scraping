{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86d12f64",
   "metadata": {},
   "source": [
    "- [ctrip](#ctrip)\n",
    "- [cenews](#cenews)\n",
    "- [people](#people)\n",
    "- [mee](#mee)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0fd91c",
   "metadata": {},
   "source": [
    "## ctrip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e8803d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import time\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eee941a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "parks = {\n",
    "    # 福建\n",
    "    \"武夷山-test\": 126481,\n",
    "    # 海南\n",
    "    # \"吊罗山\": 21835,\n",
    "    # \"尖峰岭\": 3233,\n",
    "    # \"黎母山\": 18195,\n",
    "    # \"鹦哥岭\": 145248513,\n",
    "    # \"五指山\": 3211,\n",
    "    # \"五指山水满河\": 110178,\n",
    "\n",
    "    # \"霸王岭\": 1730760,\n",
    "    # \"霸王岭雅加\": 143727908,\n",
    "    # \"霸王岭白石潭\": 143748445,\n",
    "    # \"maorui\": ,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa49fd83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_ctrip(\n",
    "        total_pages: int = 302, \n",
    "        spot_name: str = \"武夷山\",\n",
    "        spot_id: int = 126481,\n",
    "    ) -> list:\n",
    "    # Store all collected data\n",
    "    all_comments = []\n",
    "    \n",
    "    # Ctrip API\n",
    "    url = \"https://m.ctrip.com/restapi/soa2/13444/json/getCommentCollapseList\"\n",
    "    \n",
    "    # Headers that stay consistent ACROSS scenic spots\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:138.0) Gecko/20100101 Firefox/138.0\",\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"_fxpcqlniredt\": \"09031109310692966540\"\n",
    "        # No need to include x-traceID as it's generated per request\n",
    "    }\n",
    "    \n",
    "    # Customize request payload \n",
    "    payload = {     # 手机端参数\n",
    "        \"arg\": {\n",
    "            \"channelType\": 7,\n",
    "            \"collapseType\": 1,\n",
    "            \"commentTagId\": 0,\n",
    "            \"pageIndex\": 1,\n",
    "            \"pageSize\": 10,\n",
    "            \"pageType\": 1,\n",
    "            \"poiId\": 'null',\n",
    "            \"resourceId\": spot_id,\n",
    "            \"resourceType\": 11,\n",
    "            \"sortType\": 6,\n",
    "            \"sourceType\": 1,\n",
    "            \"starType\": 0,\n",
    "            \"videoImageSize\": \"700_392\"\n",
    "        },\n",
    "        \"contentType\": \"json\",\n",
    "        \"head\": {\n",
    "            \"auth\": \"\",\n",
    "            \"cid\": \"09031109310692966540\",\n",
    "            \"ctok\": \"\",\n",
    "            \"cver\": \"1.0\",\n",
    "            \"extension\": [\n",
    "                {\n",
    "                    \"name\": \"source\",\n",
    "                    \"value\": \"web\"\n",
    "                },\n",
    "                {\n",
    "                    \"name\": \"technology\",\n",
    "                    \"value\": \"H5\"\n",
    "                },\n",
    "                {\n",
    "                    \"name\": \"os\",\n",
    "                    \"value\": \"PC\"\n",
    "                },\n",
    "                {\n",
    "                    \"name\": \"application\",\n",
    "                    \"value\": \"\"\n",
    "                }\n",
    "            ],\n",
    "            \"lang\": \"01\",\n",
    "            \"sid\": \"8888\",\n",
    "            \"syscode\": \"09\",\n",
    "            \"xsid\": \"\"\n",
    "        }\n",
    "    }\n",
    "\n",
    "    with tqdm(total=total_pages, desc=f\"Scraping {spot_name}\", unit='page') as pbar:\n",
    "        for page in range(1, total_pages + 1):\n",
    "            # Update page number in payload\n",
    "            payload[\"arg\"][\"pageIndex\"] = page\n",
    "            response = requests.post(url, headers=headers, json=payload)\n",
    "            try:\n",
    "                # Parse the JSON directly\n",
    "                data = response.json()\n",
    "                comments = data[\"result\"]['items']\n",
    "                if not isinstance(comments, list):\n",
    "                    # print(\n",
    "                    #     f\"Expected a list of comments, \\\n",
    "                    #     got {type(comments)} for page {page}\"\n",
    "                    # )\n",
    "                    break\n",
    "                # Append comments to the list\n",
    "                all_comments.extend(comments)\n",
    "\n",
    "                pbar.set_postfix({\n",
    "                    'Collected': len(all_comments),\n",
    "                    'Current page': page,\n",
    "                    'Comments': len(comments),\n",
    "                })\n",
    "                # Add delay to avoid rate limiting\n",
    "                # time.sleep(2)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Request failed for page {page}: {e}\\n\")\n",
    "                print(f\"Response: \\n{response.status_code}\\n{response.json()}\")\n",
    "                break  # Exit loop on error\n",
    "            \n",
    "            finally:\n",
    "                pbar.update(1)\n",
    "\n",
    "    print(f\"Collected {len(all_comments)} for {spot_name} (ID: {spot_id})\")\n",
    "    return all_comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f726dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping 武夷山-test:   0%|          | 0/301 [00:00<?, ?page/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping 武夷山-test: 100%|██████████| 301/301 [02:32<00:00,  1.97page/s, Collected=3010, Current page=301, Comments=10]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected 3010 for 武夷山-test (ID: 126481)\n",
      "Saved 3010 comments to ctrip_武夷山-test.json\n"
     ]
    }
   ],
   "source": [
    "for park_name, park_id in parks.items():\n",
    "    comments = scrape_ctrip(\n",
    "        spot_name=park_name, \n",
    "        spot_id=park_id\n",
    "    )\n",
    "    # Save comments to a JSON file\n",
    "    with open(f\"ctrip_{park_name}.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(comments, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"Saved {len(comments)} comments to ctrip_{park_name}.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499e6998",
   "metadata": {},
   "source": [
    "## cenews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95c570ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected 20 articles from page 1\n",
      "Collected 20 articles from page 2\n",
      "Collected 20 articles from page 3\n",
      "Collected 20 articles from page 4\n",
      "Collected 20 articles from page 5\n",
      "Collected 20 articles from page 6\n",
      "Collected 20 articles from page 7\n",
      "Collected 20 articles from page 8\n",
      "Collected 20 articles from page 9\n",
      "Collected 20 articles from page 10\n",
      "Collected 20 articles from page 11\n",
      "Collected 20 articles from page 12\n",
      "Collected 20 articles from page 13\n",
      "Collected 20 articles from page 14\n",
      "Collected 20 articles from page 15\n",
      "Collected 20 articles from page 16\n",
      "Collected 20 articles from page 17\n",
      "Collected 20 articles from page 18\n",
      "Collected 20 articles from page 19\n",
      "Collected 20 articles from page 20\n",
      "Collected 20 articles from page 21\n",
      "Collected 20 articles from page 22\n",
      "Collected 20 articles from page 23\n",
      "Collected 20 articles from page 24\n",
      "Collected 20 articles from page 25\n",
      "Collected 20 articles from page 26\n",
      "Collected 20 articles from page 27\n",
      "Collected 20 articles from page 28\n",
      "Collected 20 articles from page 29\n",
      "Collected 20 articles from page 30\n",
      "Collected 20 articles from page 31\n",
      "Collected 20 articles from page 32\n",
      "Collected 20 articles from page 33\n",
      "Collected 20 articles from page 34\n",
      "Collected 20 articles from page 35\n",
      "Collected 20 articles from page 36\n",
      "Collected 20 articles from page 37\n",
      "Collected 20 articles from page 38\n",
      "Collected 20 articles from page 39\n",
      "Collected 20 articles from page 40\n",
      "Collected 20 articles from page 41\n",
      "Collected 20 articles from page 42\n",
      "Collected 20 articles from page 43\n",
      "Collected 20 articles from page 44\n",
      "Collected 20 articles from page 45\n",
      "Collected 20 articles from page 46\n",
      "Collected 20 articles from page 47\n",
      "Collected 20 articles from page 48\n",
      "Collected 20 articles from page 49\n",
      "Collected 20 articles from page 50\n",
      "Collected 20 articles from page 51\n",
      "Collected 20 articles from page 52\n",
      "Collected 20 articles from page 53\n",
      "Collected 20 articles from page 54\n",
      "Collected 20 articles from page 55\n",
      "Collected 20 articles from page 56\n",
      "Collected 20 articles from page 57\n",
      "Collected 20 articles from page 58\n",
      "Collected 20 articles from page 59\n",
      "Collected 20 articles from page 60\n",
      "Collected 20 articles from page 61\n",
      "Collected 20 articles from page 62\n",
      "Collected 20 articles from page 63\n",
      "Collected 20 articles from page 64\n",
      "Collected 20 articles from page 65\n",
      "Collected 20 articles from page 66\n",
      "Collected 20 articles from page 67\n",
      "Collected 20 articles from page 68\n",
      "Collected 20 articles from page 69\n",
      "Collected 20 articles from page 70\n",
      "Collected 20 articles from page 71\n",
      "Collected 20 articles from page 72\n",
      "Collected 20 articles from page 73\n",
      "Collected 20 articles from page 74\n",
      "Collected 20 articles from page 75\n",
      "Collected 20 articles from page 76\n",
      "Collected 20 articles from page 77\n",
      "Collected 20 articles from page 78\n",
      "Collected 20 articles from page 79\n",
      "Collected 20 articles from page 80\n",
      "Collected 20 articles from page 81\n",
      "Collected 20 articles from page 82\n",
      "Collected 20 articles from page 83\n",
      "Collected 20 articles from page 84\n",
      "Collected 20 articles from page 85\n",
      "Collected 20 articles from page 86\n",
      "Collected 20 articles from page 87\n",
      "Collected 20 articles from page 88\n",
      "Collected 20 articles from page 89\n",
      "Collected 20 articles from page 90\n",
      "Collected 20 articles from page 91\n",
      "Collected 20 articles from page 92\n",
      "Collected 20 articles from page 93\n",
      "Collected 20 articles from page 94\n",
      "Collected 20 articles from page 95\n",
      "Collected 20 articles from page 96\n",
      "Collected 20 articles from page 97\n",
      "Collected 20 articles from page 98\n",
      "Collected 20 articles from page 99\n",
      "Collected 20 articles from page 100\n",
      "Collected 20 articles from page 101\n",
      "Collected 20 articles from page 102\n",
      "Collected 20 articles from page 103\n",
      "Collected 20 articles from page 104\n",
      "Collected 20 articles from page 105\n",
      "Collected 20 articles from page 106\n",
      "Collected 20 articles from page 107\n",
      "Collected 20 articles from page 108\n",
      "Collected 20 articles from page 109\n",
      "Collected 20 articles from page 110\n",
      "Collected 20 articles from page 111\n",
      "Collected 20 articles from page 112\n",
      "Collected 20 articles from page 113\n",
      "Collected 20 articles from page 114\n",
      "Collected 20 articles from page 115\n",
      "Collected 20 articles from page 116\n",
      "Collected 20 articles from page 117\n",
      "Collected 20 articles from page 118\n",
      "Collected 20 articles from page 119\n",
      "Collected 20 articles from page 120\n",
      "Collected 20 articles from page 121\n",
      "Collected 20 articles from page 122\n",
      "Collected 20 articles from page 123\n",
      "Collected 20 articles from page 124\n",
      "Collected 20 articles from page 125\n",
      "Collected 20 articles from page 126\n",
      "Collected 20 articles from page 127\n",
      "Collected 20 articles from page 128\n",
      "Collected 20 articles from page 129\n",
      "Collected 20 articles from page 130\n",
      "Collected 20 articles from page 131\n",
      "Collected 20 articles from page 132\n",
      "Collected 20 articles from page 133\n",
      "Collected 20 articles from page 134\n",
      "Collected 20 articles from page 135\n",
      "Collected 20 articles from page 136\n",
      "Collected 20 articles from page 137\n",
      "Collected 20 articles from page 138\n",
      "Collected 20 articles from page 139\n",
      "Collected 20 articles from page 140\n",
      "Collected 20 articles from page 141\n",
      "Collected 20 articles from page 142\n",
      "Collected 20 articles from page 143\n",
      "Collected 20 articles from page 144\n",
      "Collected 20 articles from page 145\n",
      "Collected 20 articles from page 146\n",
      "Collected 20 articles from page 147\n",
      "Collected 20 articles from page 148\n",
      "Collected 20 articles from page 149\n",
      "Collected 20 articles from page 150\n",
      "Collected 20 articles from page 151\n",
      "Collected 20 articles from page 152\n",
      "Collected 20 articles from page 153\n",
      "Collected 20 articles from page 154\n",
      "Collected 20 articles from page 155\n",
      "Collected 20 articles from page 156\n",
      "Collected 20 articles from page 157\n",
      "Collected 20 articles from page 158\n",
      "Collected 20 articles from page 159\n",
      "Collected 20 articles from page 160\n",
      "Collected 20 articles from page 161\n",
      "Collected 20 articles from page 162\n",
      "Collected 20 articles from page 163\n",
      "Collected 20 articles from page 164\n",
      "Collected 20 articles from page 165\n",
      "Collected 20 articles from page 166\n",
      "Collected 20 articles from page 167\n",
      "Collected 20 articles from page 168\n",
      "Collected 20 articles from page 169\n",
      "Collected 20 articles from page 170\n",
      "Collected 20 articles from page 171\n",
      "Collected 20 articles from page 172\n",
      "Collected 20 articles from page 173\n",
      "Collected 20 articles from page 174\n",
      "Collected 20 articles from page 175\n",
      "Collected 20 articles from page 176\n",
      "Collected 20 articles from page 177\n",
      "Collected 20 articles from page 178\n",
      "Collected 20 articles from page 179\n",
      "Collected 20 articles from page 180\n",
      "Collected 20 articles from page 181\n",
      "Collected 20 articles from page 182\n",
      "Collected 20 articles from page 183\n",
      "Collected 20 articles from page 184\n",
      "Collected 20 articles from page 185\n",
      "Collected 20 articles from page 186\n",
      "Collected 20 articles from page 187\n",
      "Collected 20 articles from page 188\n",
      "Collected 20 articles from page 189\n",
      "Collected 20 articles from page 190\n",
      "Collected 20 articles from page 191\n",
      "Collected 20 articles from page 192\n",
      "Collected 20 articles from page 193\n",
      "Collected 20 articles from page 194\n",
      "Collected 20 articles from page 195\n",
      "Collected 20 articles from page 196\n",
      "Collected 20 articles from page 197\n",
      "Collected 20 articles from page 198\n",
      "Collected 20 articles from page 199\n",
      "Collected 20 articles from page 200\n",
      "Collected 20 articles from page 201\n",
      "Collected 20 articles from page 202\n",
      "Collected 20 articles from page 203\n",
      "Collected 20 articles from page 204\n",
      "Collected 20 articles from page 205\n",
      "Collected 20 articles from page 206\n",
      "Collected 20 articles from page 207\n",
      "Collected 20 articles from page 208\n",
      "Collected 20 articles from page 209\n",
      "Collected 20 articles from page 210\n",
      "Collected 20 articles from page 211\n",
      "Collected 20 articles from page 212\n",
      "Collected 20 articles from page 213\n",
      "Collected 20 articles from page 214\n",
      "Collected 20 articles from page 215\n",
      "Collected 20 articles from page 216\n",
      "Collected 20 articles from page 217\n",
      "Collected 20 articles from page 218\n",
      "Collected 20 articles from page 219\n",
      "Collected 20 articles from page 220\n",
      "Collected 20 articles from page 221\n",
      "Collected 20 articles from page 222\n",
      "Collected 20 articles from page 223\n",
      "Collected 20 articles from page 224\n",
      "Collected 20 articles from page 225\n",
      "Collected 20 articles from page 226\n",
      "Collected 20 articles from page 227\n",
      "Collected 20 articles from page 228\n",
      "Collected 20 articles from page 229\n",
      "Collected 20 articles from page 230\n",
      "Collected 20 articles from page 231\n",
      "Collected 20 articles from page 232\n",
      "Collected 20 articles from page 233\n",
      "Collected 20 articles from page 234\n",
      "Collected 20 articles from page 235\n",
      "Collected 20 articles from page 236\n",
      "Collected 20 articles from page 237\n",
      "Collected 20 articles from page 238\n",
      "Collected 20 articles from page 239\n",
      "Collected 20 articles from page 240\n",
      "Collected 20 articles from page 241\n",
      "Collected 20 articles from page 242\n",
      "Collected 20 articles from page 243\n",
      "Collected 20 articles from page 244\n",
      "Collected 20 articles from page 245\n",
      "Collected 20 articles from page 246\n",
      "Collected 20 articles from page 247\n",
      "Collected 20 articles from page 248\n",
      "Collected 20 articles from page 249\n",
      "Collected 20 articles from page 250\n",
      "Collected 20 articles from page 251\n",
      "Collected 20 articles from page 252\n",
      "Collected 20 articles from page 253\n",
      "Collected 20 articles from page 254\n",
      "Collected 20 articles from page 255\n",
      "Collected 20 articles from page 256\n",
      "Collected 20 articles from page 257\n",
      "Collected 20 articles from page 258\n",
      "Collected 20 articles from page 259\n",
      "Collected 20 articles from page 260\n",
      "Collected 20 articles from page 261\n",
      "Collected 20 articles from page 262\n",
      "Collected 20 articles from page 263\n",
      "Collected 20 articles from page 264\n",
      "Collected 20 articles from page 265\n",
      "Collected 20 articles from page 266\n",
      "Collected 20 articles from page 267\n",
      "Collected 20 articles from page 268\n",
      "Collected 20 articles from page 269\n",
      "Collected 20 articles from page 270\n",
      "Collected 20 articles from page 271\n",
      "Collected 20 articles from page 272\n",
      "Collected 20 articles from page 273\n",
      "Collected 20 articles from page 274\n",
      "Collected 20 articles from page 275\n",
      "Collected 20 articles from page 276\n",
      "Collected 20 articles from page 277\n",
      "Collected 20 articles from page 278\n",
      "Collected 20 articles from page 279\n",
      "Collected 20 articles from page 280\n",
      "Collected 20 articles from page 281\n",
      "Collected 20 articles from page 282\n",
      "Collected 20 articles from page 283\n",
      "Collected 20 articles from page 284\n",
      "Collected 20 articles from page 285\n",
      "Collected 20 articles from page 286\n",
      "Collected 20 articles from page 287\n",
      "Collected 20 articles from page 288\n",
      "Collected 20 articles from page 289\n",
      "Collected 20 articles from page 290\n",
      "Collected 20 articles from page 291\n",
      "Collected 20 articles from page 292\n",
      "Collected 20 articles from page 293\n",
      "Collected 20 articles from page 294\n",
      "Collected 20 articles from page 295\n",
      "Collected 20 articles from page 296\n",
      "Collected 20 articles from page 297\n",
      "Collected 20 articles from page 298\n",
      "Collected 20 articles from page 299\n",
      "Collected 20 articles from page 300\n"
     ]
    }
   ],
   "source": [
    "def scrape_cenews(total_pages=300):\n",
    "    articles = []\n",
    "    url = \"https://cmsapi.cenews.com.cn/cmsapi/searchArticle\"\n",
    "\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    payload = {\n",
    "        \"keyWord\": \"生态文明\",\n",
    "        \"searchLocation\": \"0\",\n",
    "        \"searchType\": \"0\",\n",
    "        \"beginTime\": \"2008-01-01 00:00:00\",\n",
    "        \"endTime\": \"2025-05-01 23:59:59\",\n",
    "        \"orderType\": \"1\",\n",
    "        \"pageNumber\": \"1\",\n",
    "        \"pageSize\": \"20\",\n",
    "        \"columnID\": \"-1\",\n",
    "        \"subSiteID\": \"1\",\n",
    "        \"siteID\": \"-1\",\n",
    "        \"includeSubNode\": \"1\",\n",
    "        \"total\": \"39589\"\n",
    "    }\n",
    "\n",
    "    for page in range(1, total_pages + 1):\n",
    "        payload[\"pageNumber\"] = str(page)\n",
    "        \n",
    "        response = requests.post(url, headers=headers, json=payload)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            if \"list\" in data:\n",
    "                articles.extend(data[\"list\"])\n",
    "                print(f\"Collected {len(data['list'])} articles from page {page}\")\n",
    "                \n",
    "                # Check if we have reached the last page\n",
    "                if len(data[\"list\"]) < 20:\n",
    "                    print(\"Reached last page\")\n",
    "                    break\n",
    "        else:\n",
    "            print(f\"Failed to fetch page {page}: {response.status_code}\")\n",
    "        \n",
    "        time.sleep(2)\n",
    "    return articles\n",
    "\n",
    "# Execute the function\n",
    "articles = scrape_cenews()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d6db318",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total articles collected: 6000\n"
     ]
    }
   ],
   "source": [
    "# Save the articles to a JSON file\n",
    "with open(\"cenews_articles.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(articles, f, ensure_ascii=False, indent=2)\n",
    "print(f\"Total articles collected: {len(articles)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85138814",
   "metadata": {},
   "source": [
    "## people"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f2160d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_people(total_pages=100):\n",
    "    articles = []\n",
    "    url = \"http://search.people.cn/search-platform/front/search\"\n",
    "\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    payload = {\n",
    "        \"endTime\": 0,\n",
    "        \"hasContent\": 'true',\n",
    "        \"hasTitle\": 'true',\n",
    "        \"isFuzzy\": 'true',\n",
    "        \"key\": \"生态文明\",\n",
    "        \"limit\": 10,\n",
    "        \"page\": 1,\n",
    "        \"sortType\": 2,\n",
    "        \"startTime\": 0,\n",
    "        \"type\": 0,\n",
    "    }\n",
    "    \n",
    "    for page in range(1, total_pages + 1):\n",
    "        payload[\"page\"] = page\n",
    "        response = requests.post(url, headers=headers,json=payload)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            records = data[\"data\"][\"records\"]\n",
    "            if not records:\n",
    "                print(f\"No records found for page {page}\")\n",
    "                break\n",
    "            articles.extend(records)\n",
    "            print(f\"Collected {len(records)} articles from page {page}\")\n",
    "            \n",
    "            # Check if we have reached the last page\n",
    "            if len(records) < 10:\n",
    "                print(\"Reached last page\")\n",
    "                break\n",
    "        else:\n",
    "            print(f\"Failed to fetch page {page}: {response.status_code}\")\n",
    "        \n",
    "        time.sleep(2)\n",
    "    return articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "97323941",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected 10 articles from page 1\n",
      "Failed to fetch page 2: 403\n",
      "Collected 10 articles from page 3\n",
      "Failed to fetch page 4: 403\n",
      "Collected 10 articles from page 5\n",
      "Failed to fetch page 6: 403\n",
      "Collected 10 articles from page 7\n",
      "Failed to fetch page 8: 403\n",
      "Failed to fetch page 9: 403\n",
      "Collected 10 articles from page 10\n",
      "Collected 10 articles from page 11\n",
      "Collected 10 articles from page 12\n",
      "Failed to fetch page 13: 403\n",
      "Collected 10 articles from page 14\n",
      "Failed to fetch page 15: 403\n",
      "Collected 10 articles from page 16\n",
      "Collected 10 articles from page 17\n",
      "Collected 10 articles from page 18\n",
      "Collected 10 articles from page 19\n",
      "Failed to fetch page 20: 403\n",
      "Collected 10 articles from page 21\n",
      "Collected 10 articles from page 22\n",
      "Collected 10 articles from page 23\n",
      "Collected 10 articles from page 24\n",
      "Collected 10 articles from page 25\n",
      "Collected 10 articles from page 26\n",
      "Collected 10 articles from page 27\n",
      "Collected 10 articles from page 28\n",
      "Collected 10 articles from page 29\n",
      "Collected 10 articles from page 30\n",
      "Collected 10 articles from page 31\n",
      "Collected 10 articles from page 32\n",
      "Collected 10 articles from page 33\n",
      "Collected 10 articles from page 34\n",
      "Collected 10 articles from page 35\n",
      "Collected 10 articles from page 36\n",
      "Collected 10 articles from page 37\n",
      "Collected 10 articles from page 38\n",
      "Collected 10 articles from page 39\n",
      "Collected 10 articles from page 40\n",
      "Collected 10 articles from page 41\n",
      "Collected 10 articles from page 42\n",
      "Collected 10 articles from page 43\n",
      "Collected 10 articles from page 44\n",
      "Collected 10 articles from page 45\n",
      "Collected 10 articles from page 46\n",
      "Collected 10 articles from page 47\n",
      "Collected 10 articles from page 48\n",
      "Collected 10 articles from page 49\n",
      "Collected 10 articles from page 50\n",
      "Collected 10 articles from page 51\n",
      "Collected 10 articles from page 52\n",
      "Collected 10 articles from page 53\n",
      "Collected 10 articles from page 54\n",
      "Collected 10 articles from page 55\n",
      "Collected 10 articles from page 56\n",
      "Collected 10 articles from page 57\n",
      "Collected 10 articles from page 58\n",
      "Collected 10 articles from page 59\n",
      "Collected 10 articles from page 60\n",
      "Collected 10 articles from page 61\n",
      "Collected 10 articles from page 62\n",
      "Collected 10 articles from page 63\n",
      "Collected 10 articles from page 64\n",
      "Collected 10 articles from page 65\n",
      "Collected 10 articles from page 66\n",
      "Collected 10 articles from page 67\n",
      "Collected 10 articles from page 68\n",
      "Collected 10 articles from page 69\n",
      "Collected 10 articles from page 70\n",
      "Collected 10 articles from page 71\n",
      "Collected 10 articles from page 72\n",
      "Collected 10 articles from page 73\n",
      "Collected 10 articles from page 74\n",
      "Collected 10 articles from page 75\n",
      "Collected 10 articles from page 76\n",
      "Collected 10 articles from page 77\n",
      "Collected 10 articles from page 78\n",
      "Collected 10 articles from page 79\n",
      "Collected 10 articles from page 80\n",
      "Collected 10 articles from page 81\n",
      "Collected 10 articles from page 82\n",
      "Collected 10 articles from page 83\n",
      "Collected 10 articles from page 84\n",
      "Collected 10 articles from page 85\n",
      "Collected 10 articles from page 86\n",
      "Collected 10 articles from page 87\n",
      "Collected 10 articles from page 88\n",
      "Collected 10 articles from page 89\n",
      "Collected 10 articles from page 90\n",
      "Collected 10 articles from page 91\n",
      "Collected 10 articles from page 92\n",
      "Collected 10 articles from page 93\n",
      "Collected 10 articles from page 94\n",
      "Collected 10 articles from page 95\n",
      "Collected 10 articles from page 96\n",
      "Collected 10 articles from page 97\n",
      "Collected 10 articles from page 98\n",
      "Collected 10 articles from page 99\n",
      "Collected 10 articles from page 100\n"
     ]
    }
   ],
   "source": [
    "people_articles = scrape_people()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5cb65192",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total articles collected: 920\n"
     ]
    }
   ],
   "source": [
    "# Save the articles to a JSON file\n",
    "with open(\"people_articles.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(people_articles, f, ensure_ascii=False, indent=2)\n",
    "print(f\"Total articles collected: {len(people_articles)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a732adff",
   "metadata": {},
   "source": [
    "## mee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be06be02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in /home/ja/.venvs/webscrape/lib/python3.12/site-packages (4.67.1)\n",
      "Requirement already satisfied: selectolax in /home/ja/.venvs/webscrape/lib/python3.12/site-packages (0.3.29)\n"
     ]
    }
   ],
   "source": [
    "!pip install tqdm selectolax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dbd14502",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from selectolax.parser import HTMLParser\n",
    "import requests\n",
    "import time\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d397842e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_mee(total_pages=30):\n",
    "    articles = []\n",
    "    base_url = \"https://www.mee.gov.cn/was5/web/search?\"\n",
    "\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:138.0) Gecko/20100101 Firefox/138.0\",\n",
    "        \"Content-Type\": 'text/html; charset=UTF-8'\n",
    "    }\n",
    "\n",
    "    params = {\n",
    "        \"channelid\": \"270514\",\n",
    "        \"searchword\": \"\",\n",
    "        \"page\": \"1\",\n",
    "        \"orderby\": \"-docreltime\",\n",
    "        \"searchscope\": \"\",\n",
    "        \"timestart\": \"\",\n",
    "        \"timeend\": \"\",\n",
    "        \"period\": \"\",\n",
    "        \"chnls\": \"3\",\n",
    "        \"andsen\": \"\",\n",
    "        \"total\": \"生态文明\",\n",
    "        \"orsen\": \"\",\n",
    "        \"exclude\": \"\"\n",
    "\t}\n",
    "\n",
    "    with tqdm(total=total_pages, desc=\"Scraping MEE\", unit='page') as pbar:\n",
    "        for page in range(1, total_pages + 1):\n",
    "            try:\n",
    "                response = requests.get(base_url, headers=headers, params={**params, \"page\": page})\n",
    "                if response.status_code == 200:\n",
    "                    tree = HTMLParser(response.text)\n",
    "                    items = tree.css('li.li')\n",
    "                    page_articles = []\n",
    "                    for item in items:\n",
    "                        url = item.css_first('a').attrs['href']\n",
    "                        category = item.css_first('em').text(strip=True)\n",
    "                        title = item.css_first('h2').text().replace(category, '').strip()\n",
    "                        date = item.css_first('span').text(strip=True)\n",
    "                        page_articles.append({\n",
    "                            \"title\": title,\n",
    "                            \"url\": url,\n",
    "                            \"category\": category,\n",
    "                            \"date\": date\n",
    "                        })\n",
    "                    articles.extend(page_articles)\n",
    "\n",
    "                    pbar.set_postfix({\n",
    "                        'Collected': len(articles),\n",
    "                        'Current': len(page_articles),\n",
    "                    })\n",
    "                time.sleep(2)  # Respectful delay between requests\n",
    "            \n",
    "            except Exception as e:\n",
    "                tqdm.write(f\"Error on page {page}: {str(e)}\")\n",
    "            \n",
    "            finally:\n",
    "                pbar.update(1)\n",
    "    \n",
    "    return articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ef8eb470",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping MEE: 100%|██████████| 30/30 [01:31<00:00,  3.06s/page, Collected=261, Current=0] \n"
     ]
    }
   ],
   "source": [
    "mee_articles = scrape_mee()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dc8831bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total articles collected: 261\n"
     ]
    }
   ],
   "source": [
    "# Save the articles to a JSON file\n",
    "with open(\"mee_政策文件.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(mee_articles, f, ensure_ascii=False, indent=2)\n",
    "print(f\"Total articles collected: {len(mee_articles)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "webscrape",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
